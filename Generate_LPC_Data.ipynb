{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LPCs From Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Declare Audio Processing Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of LPC Coefficients to Model\n",
    "NUM_LPC = 14\n",
    "\n",
    "# Sample Rate is 44.1 kHz\n",
    "RECORDED_SAMPLE_RATE = 44100\n",
    "\n",
    "# Desired Sample Rate for LPC Processing is 14 kHz\n",
    "DESIRED_SAMPLE_RATE = NUM_LPC * 1000\n",
    "\n",
    "# Front and Back Vowels\n",
    "FRONT_VOWELS = [\"iy\", \"ih\", \"eh\"]\n",
    "BACK_VOWELS = [\"uw\", \"ow\", \"aa\"]\n",
    "\n",
    "# Wav File Directory\n",
    "WAV_FILE_DIRECTORY = \"./wav_data/\"\n",
    "\n",
    "# Unit Convsersions\n",
    "SEC_TO_MS = 1/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Load Audio and Preprocess Audio\n",
    "1. Downsampling 44kHz to 14kHz (1kHz per LPC Value)\n",
    "2. Use a High Frequency Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./wav_data/vowels_Maria/aa_maria.wav\n",
    "def process_audio(path_to_audio_file: str, NUM_MS=50, HIGH_FREQUENCY_HEURISTIC=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Processes an audio file by loading it into a numpy array\n",
    "    and changing the sample rate to NUM_LPC * 1000 and slicing out \n",
    "    NUM_MS (number of miliseconds) before and after the file midpoint. \n",
    "        Note: There will be NUM_MS*2 ms worth of sampling\n",
    "\n",
    "    Additionally, there is a heuristic for adding back frequencies\n",
    "    using a simple `diff` of consecutive elements which can\n",
    "    be disabled using the HIGH_FREQUENCY_HEURISTIC flag.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_audio_file : str\n",
    "        Path to the audio input file\n",
    "\n",
    "    HIGH_FREQUENCY_HEURISTIC : bool\n",
    "        Whether or not to use transform the audio file into a difference\n",
    "        of consecutive samples. a[i] = a[i+1] - a[i] \n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    audio_array : np.ndarray\n",
    "        processed audio file as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1) Load in Raw Audio\n",
    "    audio_file, _ = librosa.load(\n",
    "        path=path_to_audio_file,\n",
    "        sr=RECORDED_SAMPLE_RATE\n",
    "    )\n",
    "\n",
    "    # Step 2) Take only the samples NUM_SEC ms before and after the midpoint of the file\n",
    "    file_middle_idx = len(audio_file) // 2\n",
    "    num_samples = int(NUM_MS * SEC_TO_MS * RECORDED_SAMPLE_RATE)\n",
    "        # Note: Sample rate is in Hz or (samples/sec), so we get # of samples from `ms * (sec/ms) * (samples/sec)`\n",
    "    middle_audio_only = audio_file[\n",
    "        file_middle_idx - num_samples:\n",
    "        file_middle_idx + num_samples\n",
    "    ]\n",
    "\n",
    "    # Step 3) Change sample rate\n",
    "    down_sampled_audio_file = librosa.resample(\n",
    "        middle_audio_only,\n",
    "        orig_sr=RECORDED_SAMPLE_RATE,\n",
    "        target_sr=DESIRED_SAMPLE_RATE\n",
    "    )\n",
    "\n",
    "    # Step 4) Rentroduce high frequencies if needed\n",
    "    if (HIGH_FREQUENCY_HEURISTIC):\n",
    "        down_sampled_audio_file = np.diff(down_sampled_audio_file)\n",
    "\n",
    "    return down_sampled_audio_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Cohen's D\n",
    "- Calculate the effect size between 2 given LPC Coefficients\n",
    "\n",
    "#### Cohen's D Intuition\n",
    "- Cohen's D = measure of the `average difference` between 2 datasets and how prominent that variation is\n",
    "- The lower the `normalized_variance` the more impactful the `mean_difference` is\n",
    "\n",
    "#### Cohen's D Calculation\n",
    "- Cohen's D = `mean_difference` / `normalized_variance`\n",
    "- let `mean_difference` = avg(`dataset1's LPC Coefficient X`) - avg(`dataset2's LPC Coefficient X`)\n",
    "- let `normalized_variance` = sqrt(`variance`) / 2\n",
    "- let `variance` = std(`dataset1's LPC Coefficient X`)^2 - std(`dataset2's LPC Coefficient X`)^2\n",
    "\n",
    "#### Cohen's D Ranges\n",
    "\n",
    "| Effect Size | `Cohen's D` Value |\n",
    "| ----------- | ----------------- |\n",
    "| Small       | <= 0.2            |\n",
    "| Medium      | <= 0.5            |\n",
    "| Large       | >= 0.8            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cohens_d(lpc_x_dataset1: np.array, lpc_x_dataset2: np.array) -> tuple[np.float32, str]:\n",
    "    \"\"\"\n",
    "    Compare the effect size of dataset1's and dataset2's LPC_X where\n",
    "    X is an arbitrary LPC Coefficient\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lpc_x_dataset1 : np.array\n",
    "        The Xth LPC coefficients for every sample in dataset 1\n",
    "            ex: LPC 12 for all `front vowels`\n",
    "    \n",
    "    lpc_x_dataset2 : np.array\n",
    "        The Xth LPC coefficients for every sample in dataset 2\n",
    "            ex: LPC 12 for all `front vowels`\n",
    "    \n",
    "    Note: Both datasets should be have the same LPC index\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    cohens_d : np.float64\n",
    "        Cohen's D or Effect Size of LPC Index X with respect to dataset 1 and dataset 2\n",
    "            ex: How likely the difference between LPC X in dataset 1 and dataset 2 \n",
    "                is to be noticeable in an experiment/real life scenario\n",
    "\n",
    "    label: str\n",
    "        Returns whether the effect size is Small, Medium, or Large\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(lpc_x_dataset1) > 1, \"lpc_x_dataset1 must have more than 1 number\" \n",
    "    assert len(lpc_x_dataset1) > 1, \"lpc_x_dataset2 must have more than 1 number\" \n",
    "\n",
    "    # Step 1) Numerator = Mean Difference\n",
    "    mean_difference = np.mean(lpc_x_dataset1) - np.mean(lpc_x_dataset2)\n",
    "    \n",
    "    # Step 2) Get the total variance of both datasets\n",
    "    total_variance = np.std(lpc_x_dataset1)**2 + np.std(lpc_x_dataset2)**2\n",
    "\n",
    "    # Step 3) Normalize the Variance\n",
    "    normalized_variance = np.sqrt(total_variance) / 2\n",
    "\n",
    "    # Step 4) Calculate effect size\n",
    "    cohens_d = mean_difference / normalized_variance\n",
    "\n",
    "    # Step 5) Get the label according to Cohen's D\n",
    "    label = \"\"\n",
    "    if(cohens_d <= 2.0):\n",
    "        label = \"Small\"\n",
    "    elif(cohens_d <= 5.0):\n",
    "        label = \"Medium\"\n",
    "    else:\n",
    "        label = \"Large\"\n",
    "\n",
    "    return cohens_d, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Vowel Lists\n",
    "- Get all files of a specific vowel type\n",
    "-   Note: This relies on a naming convention where all files belong to `./wav_data` and have the vowel name in their filename:\n",
    "-   ex: `./wav_data/*/iy*.wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_wavs_audio(WAV_DATA_PATH: str, VOWEL_LIST: list[str]) -> tuple[list[np.ndarray], list[str]]: \n",
    "    \"\"\"\n",
    "    Returns (1) a list of processed audio files and (2) its corresponding path \n",
    "    which have a match in `VOWEL_LIST`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    WAV_DATA_PATH : str\n",
    "        The root path to the wav data, need not be in this repository.\n",
    "    \n",
    "    VOWEL_LIST : list[str]\n",
    "        A list of vowels which should be included in the all_paths. Used for \n",
    "        string matching.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    all_paths : list[np.array]\n",
    "        an array of audio files processed using the `process_audio` function\n",
    "\n",
    "    all_paths : list[str]\n",
    "        an array where each index is the relative path to the wav file  \n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1) Get path to all subfolders where the wav files live    \n",
    "    all_paths = []\n",
    "\n",
    "    for vowel in VOWEL_LIST:\n",
    "        # Note: ** means a recursive search using the `glob`al search library\n",
    "        path_to_vowel = os.path.join(WAV_DATA_PATH, '**', vowel)\n",
    "        all_paths += glob.glob(f\"{path_to_vowel}*.wav\")\n",
    "    \n",
    "    # Step 2) Convert each wav file to an audio file\n",
    "    all_audio = []\n",
    "    for path in all_paths:\n",
    "        audio_file = process_audio(path_to_audio_file=path)\n",
    "        all_audio.append(audio_file)\n",
    "\n",
    "    # Step 2) Return all paths\n",
    "    return all_audio, all_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d) Aggregate LPC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 13)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m         all_lpcs\u001b[38;5;241m.\u001b[39minsert(lpc_vals)\n\u001b[1;32m     11\u001b[0m front_vowel_audio, _ \u001b[38;5;241m=\u001b[39m get_all_wavs_audio(\n\u001b[1;32m     12\u001b[0m     WAV_DATA_PATH\u001b[38;5;241m=\u001b[39mWAV_FILE_DIRECTORY, VOWEL_LIST\u001b[38;5;241m=\u001b[39mFRONT_VOWELS)\n\u001b[0;32m---> 13\u001b[0m \u001b[43maggregate_lpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfront_vowel_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[95], line 8\u001b[0m, in \u001b[0;36maggregate_lpc\u001b[0;34m(PROCESSED_AUDIO, NUM_LPC)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m PROCESSED_AUDIO:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Step 2) Get the LPC values for the audio excluding the first\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# LPC value which is always 1.0\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     lpc_vals \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mlpc(audio, order\u001b[38;5;241m=\u001b[39mNUM_LPC)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mall_lpcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m(lpc_vals)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "def aggregate_lpc(PROCESSED_AUDIO: list[np.array], NUM_LPC) -> np.ndarray:\n",
    "    all_lpcs = np.ndarray((len(PROCESSED_AUDIO), NUM_LPC-1))\n",
    "    print(all_lpcs.shape)\n",
    "    for audio in PROCESSED_AUDIO:\n",
    "        # Step 2) Get the LPC values for the audio excluding the first\n",
    "        # LPC value which is always 1.0\n",
    "        lpc_vals = librosa.lpc(audio, order=NUM_LPC)[1:]\n",
    "        all_lpcs.insert(lpc_vals)\n",
    "\n",
    "\n",
    "front_vowel_audio, _ = get_all_wavs_audio(\n",
    "    WAV_DATA_PATH=WAV_FILE_DIRECTORY, VOWEL_LIST=FRONT_VOWELS)\n",
    "aggregate_lpc(front_vowel_audio, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Generate All LPC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1) Load in data for Front and Back Vowels\n",
    "front_vowel_audio, _ = get_all_wavs_audio(WAV_DATA_PATH=WAV_FILE_DIRECTORY, VOWEL_LIST=FRONT_VOWELS)\n",
    "back_vowel_audio, _ = get_all_wavs_audio(WAV_DATA_PATH=WAV_FILE_DIRECTORY, VOWEL_LIST=BACK_VOWELS)\n",
    "\n",
    "# Step 2) Generate LPCs for each data set \n",
    "    # (n x m np.ndarray where n = # LPC Coefficients and m = # of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_vals = librosa.lpc(process_audio(\"./wav_data/vowels_Maria/aa_maria.wav\"), order=NUM_LPC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
